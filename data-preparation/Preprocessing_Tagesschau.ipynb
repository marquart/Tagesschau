{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smoking-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import re\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "german-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iraqi-pitch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO, BytesIO\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sustainable-panic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper RNNTagger_German.py muss im gleichen Verzeichnis wie dieses Notebook sein\n",
    "from RNNTagger_German import RNNTagger_German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "injured-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "more_stopwords = ('tagesschau','willkommen','untertitelung','ndr','studio','abend','fernsehen','copyright','untertitel','sch√∂n')\n",
    "STOP_WORDS.update(more_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conventional-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prospective-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnntagger_path = \"./RNNTagger-1.2.1/RNNTagger/\" # Pfad zum RNNTagger (https://www.cis.uni-muenchen.de/~schmid/tools/RNNTagger/)\n",
    "tagger = RNNTagger_German(rnntagger_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electronic-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGESSCHAU_PATH = \"C:/Users/Aron/Documents/SoSe_2021/TextViz/Tagesschau/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-commitment",
   "metadata": {},
   "source": [
    "# Save filenames per month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rural-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH_PATTERN = re.compile(\"Tagesschau_\\d?\\d-(\\d\\d-20\\d\\d)\\.json\")\n",
    "handled = 0\n",
    "\n",
    "months = defaultdict(list)\n",
    "for file in os.listdir(TAGESSCHAU_PATH):\n",
    "    if file.endswith(\".json\") and (match := MONTH_PATTERN.search(file)):\n",
    "            months[match.group(1)].append(file)\n",
    "            handled += 1\n",
    "with open(\"C:/Users/Aron/Documents/SoSe_2021/TextViz/Tagesschau/month_files.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(months, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "focused-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Aron/Documents/SoSe_2021/TextViz/Tagesschau/month_files.json\", 'r', encoding='utf-8') as f:\n",
    "    months = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-musician",
   "metadata": {},
   "source": [
    "# Process Tokens per Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secret-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"C:/Users/Aron/Documents/SoSe_2021/TextViz/Tagesschau/Months_Filtered/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "disabled_pipeline = ('ner', 'attribute_ruler', 'lemmatizer')\n",
    "token_count = 0\n",
    "time = datetime.now()\n",
    "relax = 15\n",
    "tagset = (\"NN\", \"NE\", \"FM\", \"ADJD\", \"ADJA\", \"VVINF\")\n",
    "\n",
    "for month, files in months.items():\n",
    "    if f\"{month}.json\" in os.listdir(target_dir):\n",
    "        print(f\"{month} already tokenized\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Start tokenize {month}\")\n",
    "    month_tokens = []\n",
    "    for file in files:\n",
    "        time = datetime.now()\n",
    "        with open(os.path.join(TAGESSCHAU_PATH, file), 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        txt = data['transcript'].replace('\\n\\n', ' ').replace('\\n', ' ')\n",
    "\n",
    "        doc = nlp(txt, disable=disabled_pipeline)\n",
    "        tokens = tagger.process_tokens(doc.sents, filter_tags=tagset, stopwords=STOP_WORDS)\n",
    "        month_tokens += tokens\n",
    "        print(f\"    Got {len(tokens)} tokens for {file} in {(datetime.now()-time).seconds} seconds\")\n",
    "        print(f\"        Cool down CPU for {relax} seconds\")\n",
    "        \n",
    "        sleep(relax)\n",
    "        \n",
    "    \n",
    "    with open(os.path.join(target_dir, f\"{month}.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(month_tokens, f)\n",
    "    token_count += len(month_tokens)\n",
    "    print(f\"Finished with {month}, {len(month_tokens)} tokens, {token_count} tokens so far\")\n",
    "    \n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-science",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv64_DH",
   "language": "python",
   "name": "venv64_dh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
